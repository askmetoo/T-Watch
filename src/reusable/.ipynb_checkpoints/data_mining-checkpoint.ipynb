{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import re\n",
    "import string\n",
    "from datetime import datetime\n",
    "import findspark \n",
    "findspark.init()\n",
    "import pyspark as ps\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import array_contains, col, udf, when, coalesce, array\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF, CountVectorizer, IDFModel, StopWordsRemover\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, ParamGridBuilder, TrainValidationSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = ps.sql.SparkSession.builder \\\n",
    "            .master(\"local[4]\") \\\n",
    "            .appName('Model Training') \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "        StructField('NewIndex' , IntegerType(), False),\n",
    "        StructField('Index' , IntegerType(), False),\n",
    "        StructField('ItemID' , IntegerType(), False),\n",
    "        StructField('Sentiment' , IntegerType(), False),\n",
    "        StructField('SentimentSource' , StringType(), False),\n",
    "        StructField('SentimentText' , StringType(), False),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Path does not exist: file:/home/ubuntu/capstone/data/nokaggle-Sentiment-Analysis-Dataset.csv;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o41.csv.\n: org.apache.spark.sql.AnalysisException: Path does not exist: file:/home/ubuntu/capstone/data/nokaggle-Sentiment-Analysis-Dataset.csv;\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$14.apply(DataSource.scala:382)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$14.apply(DataSource.scala:370)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:344)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:152)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:415)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-42814aeb7241>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../../data/nokaggle-Sentiment-Analysis-Dataset.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0musers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0musers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0musers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode)\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Path does not exist: file:/home/ubuntu/capstone/data/nokaggle-Sentiment-Analysis-Dataset.csv;'"
     ]
    }
   ],
   "source": [
    "filename = \"../../data/nokaggle-Sentiment-Analysis-Dataset.csv\"\n",
    "users = spark.read.csv(filename, header=True, schema=schema)\n",
    "users.printSchema()\n",
    "users.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    words = re.sub(\"[^a-zA-Z]\", \" \", text).lower().split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- NewIndex: integer (nullable = true)\n",
      " |-- Index: integer (nullable = true)\n",
      " |-- ItemID: integer (nullable = true)\n",
      " |-- Sentiment: integer (nullable = true)\n",
      " |-- SentimentSource: string (nullable = true)\n",
      " |-- SentimentText: string (nullable = true)\n",
      " |-- Words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pp_udf = udf(preprocess, ArrayType(StringType()))\n",
    "words = users.withColumn('Words', pp_udf(users.SentimentText))\n",
    "words.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------+---------+---------------+--------------------+--------------------+--------------------+\n",
      "|NewIndex|Index|ItemID|Sentiment|SentimentSource|       SentimentText|               Words|            filtered|\n",
      "+--------+-----+------+---------+---------------+--------------------+--------------------+--------------------+\n",
      "|       0|    0|     1|        0|   Sentiment140|                 ...|[is, so, sad, for...|  [sad, apl, friend]|\n",
      "|       1|    1|     2|        0|   Sentiment140|                 ...|[i, missed, the, ...|[missed, new, moo...|\n",
      "|       2|    2|     3|        1|   Sentiment140|              omg...|[omg, its, alread...|      [omg, already]|\n",
      "|       3|    3|     4|        0|   Sentiment140|          .. Omga...|[omgaga, im, sooo...|[omgaga, im, sooo...|\n",
      "|       4|    4|     5|        0|   Sentiment140|         i think ...|[i, think, mi, bf...|[think, mi, bf, c...|\n",
      "|       5|    5|     6|        0|   Sentiment140|         or i jus...|[or, i, just, wor...|       [worry, much]|\n",
      "|       6|    6|     7|        1|   Sentiment140|       Juuuuuuuuu...|[juuuuuuuuuuuuuuu...|[juuuuuuuuuuuuuuu...|\n",
      "|       7|    7|     8|        0|   Sentiment140|       Sunny Agai...|[sunny, again, wo...|[sunny, work, tom...|\n",
      "|       8|    8|     9|        1|   Sentiment140|      handed in m...|[handed, in, my, ...|[handed, uniform,...|\n",
      "|       9|    9|    10|        1|   Sentiment140|      hmmmm.... i...|[hmmmm, i, wonder...|[hmmmm, wonder, n...|\n",
      "|      10|   10|    11|        0|   Sentiment140|      I must thin...|[i, must, think, ...|[must, think, pos...|\n",
      "|      11|   11|    12|        1|   Sentiment140|      thanks to a...|[thanks, to, all,...|[thanks, haters, ...|\n",
      "|      12|   12|    13|        0|   Sentiment140|      this weeken...|[this, weekend, h...|[weekend, sucked,...|\n",
      "|      13|   13|    14|        0|   Sentiment140|     jb isnt show...|[jb, isnt, showin...|[jb, isnt, showin...|\n",
      "|      14|   14|    15|        0|   Sentiment140|     ok thats it ...|[ok, thats, it, y...|    [ok, thats, win]|\n",
      "|      15|   15|    16|        0|   Sentiment140|    &lt;-------- ...|[lt, this, is, th...|[lt, way, feel, r...|\n",
      "|      16|   16|    17|        0|   Sentiment140|    awhhe man.......|[awhhe, man, i, m...|[awhhe, man, comp...|\n",
      "|      17|   17|    18|        1|   Sentiment140|    Feeling stran...|[feeling, strange...|[feeling, strange...|\n",
      "|      18|   18|    19|        0|   Sentiment140|    HUGE roll of ...|[huge, roll, of, ...|[huge, roll, thun...|\n",
      "|      19|   19|    20|        0|   Sentiment140|    I just cut my...|[i, just, cut, my...|[cut, beard, grow...|\n",
      "+--------+-----+------+---------+---------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#remove stop words\n",
    "remover = StopWordsRemover(inputCol=\"Words\", outputCol=\"filtered\")\n",
    "removed = remover.transform(words)\n",
    "removed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------------------+--------------------+--------------------+\n",
      "|ItemID|label|       SentimentText|               Words|            filtered|\n",
      "+------+-----+--------------------+--------------------+--------------------+\n",
      "|     1|    0|                 ...|[is, so, sad, for...|  [sad, apl, friend]|\n",
      "|     2|    0|                 ...|[i, missed, the, ...|[missed, new, moo...|\n",
      "|     3|    1|              omg...|[omg, its, alread...|      [omg, already]|\n",
      "|     4|    0|          .. Omga...|[omgaga, im, sooo...|[omgaga, im, sooo...|\n",
      "|     5|    0|         i think ...|[i, think, mi, bf...|[think, mi, bf, c...|\n",
      "|     6|    0|         or i jus...|[or, i, just, wor...|       [worry, much]|\n",
      "|     7|    1|       Juuuuuuuuu...|[juuuuuuuuuuuuuuu...|[juuuuuuuuuuuuuuu...|\n",
      "|     8|    0|       Sunny Agai...|[sunny, again, wo...|[sunny, work, tom...|\n",
      "|     9|    1|      handed in m...|[handed, in, my, ...|[handed, uniform,...|\n",
      "|    10|    1|      hmmmm.... i...|[hmmmm, i, wonder...|[hmmmm, wonder, n...|\n",
      "|    11|    0|      I must thin...|[i, must, think, ...|[must, think, pos...|\n",
      "|    12|    1|      thanks to a...|[thanks, to, all,...|[thanks, haters, ...|\n",
      "|    13|    0|      this weeken...|[this, weekend, h...|[weekend, sucked,...|\n",
      "|    14|    0|     jb isnt show...|[jb, isnt, showin...|[jb, isnt, showin...|\n",
      "|    15|    0|     ok thats it ...|[ok, thats, it, y...|    [ok, thats, win]|\n",
      "|    16|    0|    &lt;-------- ...|[lt, this, is, th...|[lt, way, feel, r...|\n",
      "|    17|    0|    awhhe man.......|[awhhe, man, i, m...|[awhhe, man, comp...|\n",
      "|    18|    1|    Feeling stran...|[feeling, strange...|[feeling, strange...|\n",
      "|    19|    0|    HUGE roll of ...|[huge, roll, of, ...|[huge, roll, thun...|\n",
      "|    20|    0|    I just cut my...|[i, just, cut, my...|[cut, beard, grow...|\n",
      "+------+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered = removed.select('ItemID', col('Sentiment').alias('label'), 'SentimentText', 'Words', 'filtered')\n",
    "filtered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ItemID: integer (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- SentimentText: string (nullable = true)\n",
      " |-- Words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- filtered: array (nullable = false)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#cast empty array \n",
    "\n",
    "fill = array().cast(\"array<string>\")\n",
    "filtered_filled = coalesce(col(\"filtered\"), fill)\n",
    "filled_featurized = filtered.withColumn(\"filtered\", filtered_filled)\n",
    "filled_featurized.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------+\n",
      "|rawFeatures                                                                                          |\n",
      "+-----------------------------------------------------------------------------------------------------+\n",
      "|(200,[38,60,193],[1.0,1.0,1.0])                                                                      |\n",
      "|(200,[15,25,81,196],[1.0,1.0,1.0,1.0])                                                               |\n",
      "|(200,[57,184],[1.0,1.0])                                                                             |\n",
      "|(200,[18,56,73,82,102,132,150,155,159,166,169,185],[1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|(200,[72,164,165,169],[1.0,1.0,1.0,1.0])                                                             |\n",
      "|(200,[124,162],[1.0,1.0])                                                                            |\n",
      "|(200,[9,190],[1.0,1.0])                                                                              |\n",
      "|(200,[25,50,101,118,127],[1.0,1.0,1.0,1.0,1.0])                                                      |\n",
      "|(200,[57,62,71,191,197],[1.0,1.0,1.0,1.0,1.0])                                                       |\n",
      "|(200,[84,98,183],[1.0,1.0,1.0])                                                                      |\n",
      "|(200,[1,23,164],[1.0,1.0,1.0])                                                                       |\n",
      "|(200,[5,30,34,138],[1.0,1.0,1.0,1.0])                                                                |\n",
      "|(200,[90,102,186],[1.0,1.0,1.0])                                                                     |\n",
      "|(200,[2,17,116,176],[1.0,1.0,1.0,1.0])                                                               |\n",
      "|(200,[50,84,116],[1.0,1.0,1.0])                                                                      |\n",
      "|(200,[115,134,159,174],[1.0,1.0,1.0,1.0])                                                            |\n",
      "|(200,[22,30,33,65,76,89,106,131,132,187],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                  |\n",
      "|(200,[31,35,41,63,77,88,162,167],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                  |\n",
      "|(200,[4,14,74,198],[1.0,1.0,1.0,1.0])                                                                |\n",
      "|(200,[27,59,63,99,141,153,157,161,196,198],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                |\n",
      "+-----------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=200)\n",
    "featurized = hashingTF.transform(filled_featurized)\n",
    "featurized.select('rawFeatures').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|features                                                                                                                                                                                                                                                                          |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|(200,[38,60,193],[3.351292892234358,2.7425253655359136,3.2069298628197354])                                                                                                                                                                                                       |\n",
      "|(200,[15,25,81,196],[3.5302978401981506,2.5787462546945012,3.806789299182284,3.230943760152062])                                                                                                                                                                                  |\n",
      "|(200,[57,184],[3.4062058773194526,3.4811037871503667])                                                                                                                                                                                                                            |\n",
      "|(200,[18,56,73,82,102,132,150,155,159,166,169,185],[3.730004761243489,3.499232427988505,3.2076348991583097,5.932750901186225,3.707378652330693,3.2164347103690063,3.1296341759598167,3.2611049833609256,2.357975445745883,4.054514900232662,3.8269361048587265,3.388715924244202])|\n",
      "|(200,[72,164,165,169],[3.2171623052256915,2.6045128481045268,3.225775376524396,3.8269361048587265])                                                                                                                                                                               |\n",
      "|(200,[124,162],[3.236187374489301,3.750669742661097])                                                                                                                                                                                                                             |\n",
      "|(200,[9,190],[3.515987632738829,3.936845597871823])                                                                                                                                                                                                                               |\n",
      "|(200,[25,50,101,118,127],[2.5787462546945012,3.111831853786621,3.1197958080613817,3.6196570984771834,2.745956847791588])                                                                                                                                                          |\n",
      "|(200,[57,62,71,191,197],[3.4062058773194526,2.607716341188582,2.945273045924273,3.5352007679067667,3.4351017754415927])                                                                                                                                                           |\n",
      "|(200,[84,98,183],[3.2116237858330487,3.6251385145864248,3.3895803747142854])                                                                                                                                                                                                      |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featurized.cache()\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "model = idf.fit(featurized)\n",
    "result = model.transform(featurized)\n",
    "result.limit(10).select('features').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save idf and idf model\n",
    "idf_path = '../tmp/idf'\n",
    "idf.save(idf_path)\n",
    "idfmodel_path = '../tmp/idfmodel'\n",
    "model.save(idfmodel_path)\n",
    "#load via following\n",
    "#loadedModel = IDFModel.load(idfmodel_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32 ms, sys: 0 ns, total: 32 ms\n",
      "Wall time: 1min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rf = RandomForestClassifier(numTrees=100, labelCol=\"label\", seed=42)\n",
    "rf_model = rf.fit(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SentimentText: string (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+----------+\n",
      "|       SentimentText|         probability|prediction|\n",
      "+--------------------+--------------------+----------+\n",
      "|                 ...|[0.50464022409798...|       0.0|\n",
      "|              omg...|[0.49997696970661...|       1.0|\n",
      "|          .. Omga...|[0.51373803403465...|       0.0|\n",
      "|     jb isnt show...|[0.49997696970661...|       1.0|\n",
      "|    awhhe man.......|[0.48503937730438...|       1.0|\n",
      "+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CPU times: user 220 ms, sys: 48 ms, total: 268 ms\n",
      "Wall time: 7min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Prepare Train Test Split\n",
    "train, test = result.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Configure an ML pipeline, which consists of tree stages: hashingTF, idf and RandomForestClassifier.\n",
    "#remover = StopWordsRemover(inputCol=\"Words\", outputCol=\"filtered\")\n",
    "#hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "rf = RandomForestClassifier(labelCol=\"label\", seed=42)\n",
    "pipeline = Pipeline(stages=[rf])\n",
    "\n",
    "#grid search\n",
    "paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [100]).addGrid(rf.maxDepth, [5]).build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=3)  # use 3+ folds in practice\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "prediction = cvModel.transform(test)\n",
    "selected = prediction.select(\"SentimentText\", \"probability\", \"prediction\")\n",
    "selected.printSchema()\n",
    "selected.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.645381227448802]\n"
     ]
    }
   ],
   "source": [
    "print (cvModel.avgMetrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
